# SurgVU 2025 Category 2 VQA

This repository provides a complete example for running `inference.py` in a local or containerized environment to perform Visual Context Question Answering (VC-QA) on endoscopic robotic surgery videos and produce standardized JSON outputs.

---

## Environment & Dependencies

- Operating System: Linux (recommended; other platforms require self-testing)
- Python Version: Compatible with `requirements.txt`
- GPU: NVIDIA GPU with appropriate CUDA drivers recommended (the model prefers CUDA when available)

Install dependencies (optional; required for local direct runs):

```bash
pip install -r requirements.txt
```

---

## Directories & Key Paths

- Input directory: `/input`
- Output directory: `/output`
- Model directory: `/opt/app/model`

For local debugging outside the container, we recommend using the repository test directories:
- Local input: `test/input`
- Local output: `test/output`
- Local model: `model`

Default paths used inside `inference.py` (effective when running in the container):
- Reads `/input/inputs.json` to determine the interface (generated by the platform)
- Expected input files:
  - `/input/endoscopic-robotic-surgery-video.mp4`
  - `/input/visual-context-question.json`
- Output file:
  - `/output/visual-context-response.json`

---

## Model Files Preparation

Download the model from ðŸ¤—[surgvu2025vqa](https://huggingface.co/bravefox12138/surgvu2025vqa)

In `load_model()`, `inference.py` reads the following from `MODEL_PATH=/opt/app/model`:
- `alg.cfg`: algorithm configuration (INI/CFG format)
- `last.pt`: Ultralytics YOLO detection model weights
- `checkpoint-399.pth`: classification model weights (ResNet50, 12 classes)
- Qwen2.5-VL model and processor directory (for `Qwen2_5_VLForConditionalGeneration` and `AutoProcessor`)

Ensure the above weights and model directory are placed under `/opt/app/model` in the container image (or switch `MODEL_PATH` to a local path for direct local runs).

---

## Input & Output Formats

- Input video: `/input/endoscopic-robotic-surgery-video.mp4`
- Input question (JSON): `/input/visual-context-question.json`
  - The content is a single string, for example:

```json
"Is suturing used in this procedure?"
```

- Output answer (JSON): `/output/visual-context-response.json`
  - The content is a single string, for example:

```json
"Yes, suturing is expected to be part of the procedure."
```

Note: The platform describes the interface of the current invocation in `/input/inputs.json`. `inference.py` parses this file to determine required inputs and select the corresponding handler.

---

## Quick Local Testing

If you want to run `inference.py` directly in your local Python environment, ensure that:
- Dependencies are installed
- Files under `test/input` are prepared according to the "Input & Output Formats"
- Modify `INPUT_PATH`/`OUTPUT_PATH`/`MODEL_PATH` at the top of `inference.py` to your local paths as needed (commented examples are provided in the file)


Local direct run example:

```bash
python inference.py
```

Upon success, the answer will be written to: `test/output/visual-context-response.json`

---

## Docker Run (Custom)

If you prefer to run the container manually (without the helper scripts), use:

```bash
docker run --rm \
  --gpus all \
  -v $(pwd)/test/input:/input:ro \
  -v $(pwd)/test/output:/output \
  your-image-name:latest
```

Ensure the image contains the required weights and Qwen2.5-VL model directory under `/opt/app/model`.

---

## Inference Flow Overview

Main flow in `inference.py`:
- `run()`: entry point
  - Read `inputs.json` and confirm the interface
  - Load models (YOLO detection, ResNet50 classification, Qwen2.5-VL)
  - Read video and question, execute `deepthink_infer_by_file(...)`
  - Normalize the answer text and write to `/output/visual-context-response.json`

When a GPU is available, `cuda:0` is used by default. Peak memory usage will be printed at the end of inference.

---

## Troubleshooting

- GPU/VRAM related errors:
  - Ensure compatible drivers and CUDA runtime are installed; if needed, reduce batch/frame sampling or switch to a smaller model.
- Missing weights:
  - Check that `MODEL_PATH` contains `alg.cfg`, `last.pt`, `checkpoint-399.pth`, and the Qwen model directory; for container runs, copy them during the image build stage.
- Empty or malformed output:
  - Verify the input question JSON is a string; ensure `visual-context-question.json` and the video file name/path are correct.
- Path mismatches when running locally:
  - Adjust `INPUT_PATH`/`OUTPUT_PATH`/`MODEL_PATH` in `inference.py` to your local testing paths (commented examples at the top of the file).
